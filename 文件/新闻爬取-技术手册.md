## python 3.8.5

## 八爪鱼采集器

- 采用第三方软件（八爪鱼采集器）爬取，初始数据（100条）
- Python用于定时更新爬取

# 爬取网站

| 网站   | 爬取一级URL                                   | name       |
| ------ | --------------------------------------------- | ---------- |
| 新华网 | http://www.xinhuanet.com/politicspro/szgz.htm | XinHuaWang |
|        |                                               |            |
|        |                                               |            |

# 功能介绍

| 功能                 | 介绍                         | 功能代码位置     |
| -------------------- | ---------------------------- | ---------------- |
| 定时爬取             | 规定时间，可每天定时定量爬取 | 主爬虫py/parse   |
| 去重                 | 去除爬取过的新闻             | 主爬虫py/parse   |
| IP代理池             | 需要付费购买IP池，防封       | S1MiddleWares.py |
| 图片存放在本地文件夹 | ~~~                          | 主爬虫py         |
|                      |                              |                  |



## 新华网 

一级URL：http://www.xinhuanet.com/politicspro/szgz.htm

| #    | 标题 | 标题_链接 | 图片 | 标签 |
| ---- | ---- | --------- | ---- | ---- |
|      |      |           |      |      |

**说明：**

- 标签：在时政标签中，也分为多个标签，如：时政、社会、廉政。





#  scrapy框架

## scrapy框架原理

### Scrapy异步框架核心原理

- 本节目标
  - 同步和异步
  - Scrapy运行原理
- 同步和异步
  - 同步：下一个方法==依赖==与上一个方法的结果
  - 异步：下一个方法==不依赖==与上一个方法的结果

- Scrapy运行原理

![image-20210518174900689](https://i.loli.net/2021/05/18/idJGw2LmOxcj4Cn.png)

### Scrapy项目创建与配置

1. Scrapy安装：pip install scrapy

2. 项目创建

   1. 创建项目

   \>> scrapy startproject [项目名字]

   2. 进入项目

   \>>cd [项目名字]

   3. 创建爬虫文件

   \>>scrapy genspider [爬虫名字] "[HOST地址]"

   4. 运行爬虫文件

   \>>scrapy crawl [爬虫名字]

3. Settings中的常用配置：

   USER_AGENT = ""								# User-Agent

   ROBOTSTXT_OBEY = True|Flase		# 是否遵循机器人协议

   DEFAULT_REQUEST_HEADERS = {}	# 默认Headers

   CONCURRENT_REQUESTS = 16		# 下载器最大处理的请求数

   DOWNLOAD_DELAY = 3					# 下载延时

   SPIDER_MIDDLEWARES					# Spid中间件

   DOWNLOADER_MIDDLEWARES		# Downloader中间件

   ITEM_PIPELINES									# 管道文件

   ---

   Spider中的属性和方法：

   ```python
   # 爬虫名字
   name = 's1'
   # 如果URL地址的HOST不属于allowed_domains，则过滤掉该请求
   allowed_domains = ['edu.csdn.net']
   # 项目启动时，访问的URL地址
   start_urls = ['http://edu.csdn.net/']
   # 访问start_urls，得到响应后调用的方法
   def parse(self, response):  # response为响应对象
       pass
   # 爬虫开始，执行的方法，相当于start_urls
   def start_requests(self):
       yield scrapy.Request(	# 向调度器发送一个Request对象
       	url = 'http://edu.csdn.net',  # 请求地址，默认Get方式
           callback = self.parse2  # 得到响应后，调用的函数
       )
   def parse2(self,response):  # 得到响应后，调用的函数
       print(response.body)	# 得到字节类型的数据
   ```

   

### Middleware中间件

### Pipeline管道文件

### Scrapy异步抓取实战

